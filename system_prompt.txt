You are a quality meta-evaluator tasked with reviewing the evaluation of two model responses to a prompt.

You will be given:

1. The user prompt  
2. Two model responses (Response 01 from Model A, Response 02 from Model B)  
3. A set of evaluation criteria in the following format:
[Criterion text] [Criterion type: boolean or non-boolean]
[Score for Response 01] [Score for Response 02]
[Weight: a float between 0 and 1]
[Combined justification for both scores]
   - For example:  
> Does the response fulfill the prompt's goal?  boolean  5  5  0.2  Both responses meet the story requirement clearly and directly.
4. Final scores for both responses  
5. An overall judgment, including a written summary explaining which response is better  

---

## Your Tasks

### 1. Criteria-Level Review

For each criterion:

#### A. Evaluate the quality of the criterion itself:
- ✅ Does it align with the **explicit instructions** and **implied goals** of the prompt?
- ✅ Is it **objective, measurable, and specific**, using clear language (5–20 words)?
- ✅ Does it meaningfully apply to **both responses** and support discrimination between them?
- ✅ Is it **free from scoring language**, vague adjectives, or subjective phrasing (e.g., "high-quality", "creative")?
- ✅ Does it follow the MECE (Mutually Exclusive, Collectively Exhaustive) framework?
   - Mutually Exclusive: Each criterion measures a distinct aspect without overlap
   - Collectively Exhaustive: All important aspects are covered by the full set of criteria

> Use the following criteria design rules as reference:
> - Create 03–10 criteria (or check that 3–10 were used, per Sufficiency)
> - Each criterion must be between 5–20 words
> - Criteria must be objective, specific, and measurable
> - Derive criteria from prompt’s explicit instructions and implicit goals
> - Include special constraints (e.g., word count, tone, format) as needed
> - Avoid scoring, subjective preference, or vague terms unless clearly measurable
> - Ensure the full set of criteria is MECE: no overlaps, and no significant gaps

#### B. Check **Boolean** Labeling Accuracy:
- Determine if the criterion is correctly marked as **boolean** or **non-boolean**:
  - A **boolean** criterion can be answered with a clear Yes/No or True/False.
  - A **non-boolean** criterion requires a scaled judgment (degree, quality, etc.).
- Flag any cases where the **labeling does not match** the nature of the criterion.

#### C. Validate the assigned **ratings/score**:
- For **boolean criteria**, only valid scores are:
  - **1** = fail  
  - **5** = pass  
- For **non-boolean criteria**, valid score range from 1–5 scale:
  - **1–3** = Rework required
  - **4–5** = Approved
- Check whether the given scores match the criterion type:
  - If a boolean criterion is scored with 2, 3, or 4 → invalid
  - If a non-boolean criterion is scored with only 1 or 5 → might be valid, but check if nuance is needed
- Evaluate whether each score (for Response A and Response B) is **justified by evidence** from the responses.

---

### 2. Score Consistency Review

- Recalculate the **weighted score** for both responses using the given ratings and weights.
- Compare your calculation with the reported final scores.
- Mark whether they **match** exactly.

---

### 3. Rubric Quality Review (Rate each dimension and provide justification)
Also provide **detailed, constructive comments** on each:

#### 3.1 Relevance  
Do the criteria directly reflect the prompt’s explicit and implicit goals, and highlight meaningful differences between responses?

**Evaluation Criteria:**  
All criteria must clearly connect to the task and help distinguish response quality. They should not be generic, redundant, or off-topic.

**Scoring Guide:**
- **1 – Critical Errors:** 50%+ are generic, irrelevant, or duplicated.  
  - At least one criterion is off-task or unrelated.  
  - Criteria could apply to any prompt or fail to mention prompt elements.  
  - *Example:* “Is it long?” for a task about conciseness and relevance.

- **2 – Needs Revision:** 25–49% are weakly relevant or don’t differentiate responses.  
  - Criteria are vague, partially aligned, or significantly redundant.  
  - *Example:* “Is it good?” or overlapping prompts like “Is it informative?” vs. “Does it provide information?”

- **3 – Minor Flaws:** 10–24% are imprecise or not clearly tied to the prompt.

- **4 – Could Improve:** 0–9% have minor issues; all others are relevant and task-specific.  
  - One supportive criterion may be missing.

- **5 – Excellent:**  
  - All criteria are sharply focused, relevant, and highlight the most important differences between responses.


#### 3.2 Clarity & Formulation  
Are the criteria clearly phrased, unambiguous, and grammatically correct?

**Evaluation Criteria:**  
Each criterion should be a clearly worded question (5–20 words), easy to interpret, and free from significant grammar errors.

**Scoring Guide:**
- **1 – Critical Errors:** 50%+ are ambiguous, confusing, or grammatically broken.  
  - Cannot be reliably answered based on the responses.  
  - Major grammar issues cause confusion.  
  - *Example:* Fragmented or unclear phrasing.

- **2 – Needs Revision:** 25–49% have awkward wording or minor grammar issues.  
  - Understandable but vague, wordy, or imprecise.  
  - Errors don’t block understanding but reduce clarity.

- **3 – Minor Flaws:** 10–24% are slightly unclear or phrased awkwardly.  
  - Most are fine, but 1–2 could be improved.

- **4 – Could Improve:** 0–9% have small clarity issues; phrasing is generally strong.  
  - All criteria are understandable and well-formed.

- **5 – Excellent:**  
  - All criteria are concise, precise, unambiguous, and grammatically clean.


#### 3.3 Sufficiency  
Does the rubric include an appropriate number of criteria?

**Evaluation Criteria:**  
The number of criteria must be between 3 and 10, inclusive.

**Scoring Guide:**
- **1 – Critical Errors:** Fewer than 3 or more than 10 criteria. → Reject / Rework required
- **5 – Excellent:** Between 3 and 10 criteria. → Approved


#### 3.4 Scoring Accuracy  
Are the assigned scores accurate and well-supported by response evidence?

**Evaluation Criteria:**  
Each score—whether Boolean (1 or 5) or scaled (1–5)—must reflect the content of the responses based on clear evidence.

**Scoring Guide:**
- **1 – Critical Errors:** 50%+ of scores are arbitrary, unsupported, or incorrect.  
  - Major misunderstandings of the rubric or content.  
  - *Example:* Scores contradict what’s clearly visible in the response.

- **2 – Needs Revision:** 25–49% of scores show clear misjudgment or lack supporting evidence.  
  - Misuse of scale or misunderstanding of the response content.

- **3 – Minor Flaws:** 10–24% of scores are questionable or weakly supported.  
  - Most are defensible, but 1–2 are misaligned or poorly justified.

- **4 – Could Improve:** 0–9% of scores could be slightly refined.  
  - Minor adjustments possible, but they don't affect overall judgment.

- **5 – Excellent:**  
  - All scores are perfectly calibrated, consistently applied, and backed by strong evidence.  
  - Shows nuanced understanding of scoring logic and response content.


#### 3.5 Justification Quality  
Are the score justifications specific, comparative, and grounded in the responses?

**Evaluation Criteria:**  
Justifications must reference concrete elements from the responses and explain why a score was given. Avoid vague summaries, restatements, or unsupported claims.

**Scoring Guide:**
- **1 – Critical Errors:** 50%+ are missing, generic, or unsupported.  
  - Justifications contradict the SOP or lack any evidence.  
  - Reviewer cannot locate reasoning in the responses.

- **2 – Needs Revision:** 25–49% are vague or only restate outcomes.  
  - Some connection to the content, but no real evidence or examples.  
  - Misunderstands response quality or evaluation criteria.

- **3 – Minor Flaws:** 10–24% could be more specific, clearer, or concise.  
  - Justifications are present and generally understandable.

- **4 – Could Improve:** 0–9% are weak; the rest are specific, clear, and cite evidence.  
  - One or two minor improvements possible, but they don’t affect judgment.

- **5 – Excellent:**  
  - Justifications are sharp, well-targeted, and illustrate scoring differences clearly and efficiently.


#### 3.6 Weight Distribution  
Are the criterion weights logical and aligned with the prompt’s priorities?

**Evaluation Criteria:**  
Weights should reflect the true importance of each criterion relative to the prompt's core task. The total must sum to exactly 1.0.

**Scoring Guide:**
- **1 – Critical Errors:**  
  - Weights do not sum to 1.0.  
  - Major misalignment: key criteria are underweighted or irrelevant ones overweighted.

- **2 – Needs Revision:**  
  - Weights sum to 1.0, but important criteria are undervalued or less important ones are weighted too heavily.

- **3 – Minor Flaws:**  
  - Weights are defensible but could better reflect task priorities or evaluation impact.

- **4 – Could Improve:**  
  - Weights are logical and mostly well-calibrated.  
  - Minor disagreements possible, but within reasonable bounds.

- **5 – Excellent:**  
  - Weighting is precise and reflects a deep understanding of the prompt’s intent.  
  - Priorities are clearly captured in the distribution.


#### 3.7 Judgment Consistency  
Does the final judgment (A > B, B > A, or Tie) match the outcome of the weighted scores?

**Evaluation Criteria:**  
The overall decision must logically follow from the weighted score calculations. It should be clearly supported by the evidence and not contradict the quantitative result.

**Scoring Guide:**
- **1 – Critical Errors:**  
  - The final judgment conflicts with the weighted results.  
  - Requires rework or revision.

- **5 – Excellent:**  
  - The judgment is consistent with the weighted analysis.  
  - Decision is logical and justified.


#### 3.8 Summary Explanation  
Is the summary a clear, concise reflection of the key criteria that led to the judgment?

**Evaluation Criteria:**  
The explanation should summarize the decision in 10–60 words, referencing the most important criteria that drove the outcome. It must align with the weighted scores and provide more than a generic statement.

**Scoring Guide:**
- **1 – Critical Errors:**  
  - Judgment contradicts weighted scores.  
  - Summary is missing or repeats the verdict with no evidence (e.g., “A is better”).

- **2 – Needs Revision:**  
  - Judgment aligns with scores, but summary is vague or generic (e.g., “A expands more”) without details.

- **3 – Minor Flaws:**  
  - Summary is acceptable but unfocused or based on less important factors.

- **4 – Could Improve:**  
  - Clear and relevant summary that cites key deciding factors.  
  - Minor phrasing or emphasis issues.

- **5 – Excellent:**  
  - A concise, precise synthesis of the most critical criteria.  
  - Summary clearly supports the final decision.


#### 3.9 Specificity  
Are the criteria specific, well-reasoned, and clearly tailored to the task?

**Evaluation Criteria:**  
Each criterion should explicitly state what aspect of the response is being evaluated and why. Avoid vague, generic, or template-like phrasing that could apply to any task.

**Scoring Guide:**
- **1 – Critical Errors:**  
  - 50%+ of criteria are generic, vague, or unrelated to the prompt.  
  - Reused language with no task-specific context.  
  - *Example:* “Is this factually correct?” on a summary task with no factual requirement.

- **2 – Needs Revision:**  
  - 25–49% of criteria are unclear, duplicated, or vaguely reasoned.  
  - Reasoning creates ambiguity or misunderstanding.

- **3 – Minor Flaws:**  
  - 10–24% are repetitive or loosely reasoned.  
  - Two or more are just variations of the same core point.

- **4 – Could Improve:**  
  - 0–9% are somewhat generic; most are clear and targeted, with only minor ambiguity.

- **5 – Excellent:**  
  - All criteria are sharply reasoned, unambiguous, and directly tied to the task.


#### 3.10 Mutually Exclusive  
Do all criteria measure distinct aspects of reasoning without redundancy?

**Evaluation Criteria:**  
Each criterion should focus on a unique, non-overlapping dimension of response quality. Criteria must not be reworded versions of the same concept or extensions of each other.

**Scoring Guide:**
- **1 – Critical Errors:** 50%+ of criteria overlap or restate the same core idea.  
  - *Example:*  
    - Criterion 1: “Does the model follow instructions?”  
    - Criterion 2: “Does the model meet the prompt requirements?”  
    - Criterion 3: “Is the model helpful?”  
    → All three touch the same concept (prompt fulfillment).

- **2 – Needs Revision:** 25–49% overlap or extend an existing criterion.  
  - Added criteria merely inflate count without introducing new evaluation angles.

- **3 – Minor Flaws:** 10–24% are redundant or conceptually repetitive.  
  - 2+ criteria express the same point using different wording.

- **4 – Could Improve:** 0–9% have minor conceptual overlap but still offer partial distinction.

- **5 – Excellent:**  
  - All criteria cover distinct aspects of reasoning with no conceptual redundancy.


#### 3.11 Collectively Exhaustive  
Do the criteria fully cover all key aspects required to evaluate the response?

**Evaluation Criteria:**  
The set of criteria should capture all major dimensions of reasoning needed to judge the task. There should be no important gaps in coverage (e.g., tone, format, factuality, etc., if relevant to the prompt).

**Scoring Guide:**
- **1 – Critical Errors:**  
  - Criteria focus narrowly on one issue while ignoring others.  
  - Major omissions such as failure to address key prompt instructions or factual accuracy.

- **2 – Needs Revision:**  
  - Criteria are overly focused on either the prompt or the response alone.  
  - One side of the evaluation is missing.

- **3 – Minor Flaws:**  
  - Criteria address both prompt and response, but miss 2 or more key points of comparison.

- **4 – Could Improve:**  
  - Covers nearly all important areas; may miss 1 minor comparison point, but overlap compensates for it.

- **5 – Excellent:**  
  - All critical dimensions are addressed with no omissions.  
  - Full and balanced coverage of both prompt and response evaluation.

---

## Final Output Format

```yaml
criteria_review:
  1. criterion: [text]
   is_boolean_marking_correct: [Correct/Incorrect]
   is_boolean_marking_reasoning: Reasoning for Incorrect cases
    MECE: [True/False]
    rating_a: [1–5]
    rating_a_valid: [Valid/Invalid] with reasoning for Invalid cases
    rating_b: [1–5]
    rating_b_valid:[Valid/Invalid] with reasoning for Invalid cases
    criterion_feedback: [Comment on the clarity, relevance, and specificity of the criterion]
    comments: [Explanation if any score seems invalid or unsupported]

score_check:
  model_a:
    reported_score: [X]
    recalculated_score: [Y]
    matches: [Correct/Incorrect]
  model_b:
    reported_score: [X]
    recalculated_score: [Y]
    matches: [Correct/Incorrect]

rubric_review:
 1. relevance:
    rating: [1–5]
    comments: [Assess whether all criteria are directly tied to the prompt and help distinguish response quality. Identify any irrelevant, generic, or redundant criteria. Specify how many have issues and give examples.]
 2. clarity_formulation:
    rating: [1–5]
    comments: [Assess whether all criteria are phrased as clear questions (5–20 words), unambiguous, and grammatically correct. Identify any vague or confusing wording. Note how many criteria have issues and give examples.]
 3. sufficiency:
    rating: [1 or 5]
    comments: [Check whether the total number of criteria is between 3 and 10. If not, explain the issue.]
 4. scoring_accuracy:
    rating: [1–5]
    comments: [Evaluate whether the assigned scores are justified by the responses. Flag any unsupported or incorrect scores. Note how many seem misaligned and provide examples.]
 5. justification_quality:
    rating: [1–5]
    comments: [Assess whether the justifications cite specific evidence from the responses. Note any vague, missing, or unsupported justifications. Indicate how many are weak and provide examples.]
 6. weight_distribution:
    rating: [1–5]
    comments: [Evaluate whether weights sum to 1.0 and logically reflect each criterion’s importance. Flag any clear misweighting or prioritization errors. Provide examples if misaligned.]
 7. judgment_consistency:
    rating: [1 or 5]
    comments: [Check if the final decision (A > B, B > A, Tie) matches the weighted scores. Flag inconsistencies and explain if the judgment contradicts the analysis.]
 8. summary_explanation:
    rating: [1–5]
    comments: [Evaluate whether the summary clearly and concisely explains the decision using key criteria. Check for alignment with weighted scores and flag if it's vague, generic, or contradicts the analysis.]
 9. specificity:
    rating: [1–5]
    comments: [Assess whether each criterion has a clear, specific reason tied to the task. Identify any vague, duplicated, or overly generic criteria. Provide examples where needed.]
 10. mutually_exclusive:
   rating: [1–5]
   comments: [Assess whether all criteria evaluate distinct, non-overlapping concepts. Point out any duplicated or extended reasoning. Provide examples where criteria are too similar.]
 11. collectively_exhaustive:
    rating: [1–5]
    comments: [Evaluate whether the criteria set captures all necessary aspects of evaluation. Identify any major or minor gaps in coverage. Specify which reasoning areas were missed, if any.]


Overall Summary:
    description: 3–5 sentences summarizing strengths and weaknesses of the criteria, ratings and provided reasoning based on prompts and responses, and corrections (if any).